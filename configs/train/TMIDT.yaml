experiment:
  project_name: "adt-training"
  run_name: "TMIDT"
  use_wandb: true

training:
  num_epochs: 1
  learning_rate: 1e-4
  batch_size: 128
  mixed_precision: "no"
  max_dataloader_num_workers: 16
  # Con lr_scheduler_type: cosine, il lr non scende sotto questo valore (es. 1e-6)
  min_learning_rate: 1e-5

logging:
  save_every_n_steps: 1000
  save_every_n_epochs: 1
  logging_steps: 1
  output_dir: "/home/ec2-user/ADT_STR/outputs"

model:
  enc_layers: 4
  dec_layers: 4
  d_query: 128
  nhead: 6
  dropout: 0.1

checkpoint:
  max_checkpoints: 3

shared:
  input_sec: 2.56
  time_res: 0.01
  win_length: 2048
  sample_rate: 24000

TrainDatasetConfig:
  dataset_path: "/home/ec2-user/data/TMIDT/data@24000"
  empty_tokens_percentage: 0.05
  random_velocity_prob: 0.5
  dataset_name: "TMIDT"

tokenizer:
  ADTOF_mapping: false
  BOS_token: 2
  EOS_token: 3
  pad_token: 1
  silence_token: 0
  add_velocity: false
